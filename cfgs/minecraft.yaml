env:
  _target_: 'envs.minecraft.Minecraft'
  shape: [10, 10]
  initial_state: [9, 2]
  slip_probability: 0.
classes:
  discrete: <class 'envs.minecraft.minecraft'>
logger:
  dir_name: minecraft
ltl:
  autobuild: False
  formula:  "G(F(wood & X(F(work_bench) & X(F(gold)))))"  #GF(work_bench & X(F(gold)))  & (G !obstacle)
  oa_type: ldba
  rabinizer: ./rabinizer4/bin/ltl2ldba
q_learning:
  batches_per_update: 20
  batch_size: 1024
  update_freq__n_episodes: 1
  temp_decay_freq__n_episodes: 25
  temp_decay_rate: .95
  min_action_temp: 0.01
  n_traj: 1000
  n_pretrain_traj: 1300
  T: 200
  init_temp: .6
  lr: .001
  iterations_per_target_update: 15
  temp_decay_type: 'exponential'
testing:
  testing_freq__n_episodes: 10
  num_rollouts: 1
gamma: .99
n_grad_updates: 1
delta: .1
n_seeds: 5
init_seed: 6
lambda: 200.0
lambda_qs: 100.0
mdp_multiplier: 1.0
replay_buffer_size: 50000
checkpoint_freq__n_episodes: 50
visualize: True
reward_type: 1
load_path: 0  # 0 if not pre-loading a model, otherwise a string path to the model.
model_name: "minecraft_norew"
run_name: "minecraft_retry"
num_eval_trajs: 50
eval_horizon: 500
baseline: "ours" # ours, baseline, no_mdp